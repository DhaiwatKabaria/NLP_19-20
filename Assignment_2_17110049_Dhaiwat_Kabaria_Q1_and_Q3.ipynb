{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NT9grp0ZRk0W"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "from collections import Counter\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.lm import MLE\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "J-HoEbEYYmmz",
    "outputId": "3fb170a3-eba8-47ae-db13-eafbcee5b956"
   },
   "outputs": [],
   "source": [
    "#accessing  drive and opening file + decoding in read mode.\n",
    "\n",
    "myfile1 = open(\"Project Gutenberg's The Complete Works of Jane Austen, by Jane Austen.txt\",\"r\")\n",
    "\n",
    "#Replace newline with space and convert to lower case\n",
    "text=myfile1.read().replace(\"\\n\",\" \")\n",
    "text=text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7ouGoTKWdosa"
   },
   "outputs": [],
   "source": [
    "#tokenizing sentences\n",
    "stopwords_en = set(stopwords.words(\"english\"))\n",
    "sents = nltk.sent_tokenize(text)\n",
    "sents_rm_stopwords = []\n",
    "for sent in sents:\n",
    "  sents_rm_stopwords.append(' '.join(w for w in nltk.word_tokenize(sent) if w.lower() not in stopwords_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kxtsYEnSo8t2"
   },
   "outputs": [],
   "source": [
    "#removing useless symbols\n",
    "process = []\n",
    "tdata=[]\n",
    "for sent in sents_rm_stopwords:\n",
    "  out = re.sub(r'[^\\w\\d\\s]+','', sent)\n",
    "  out=str(out)\n",
    "  process.append('<s> '+out+'</s>')\n",
    "  tdata.append(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ujSu1doqAukk"
   },
   "outputs": [],
   "source": [
    "#test-train split of data \n",
    "data_train, data_test = train_test_split(process, test_size=0.2, random_state=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "juPFSoOYJwlK"
   },
   "outputs": [],
   "source": [
    "nest_pro=[]\n",
    "for sent in data_train:\n",
    "    temp = re.split(r'[\" \"]+', sent)\n",
    "    for i in temp:\n",
    "        if i==\"\":\n",
    "            temp.remove(i)\n",
    "    nest_pro.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IS7T7swM0jUR"
   },
   "outputs": [],
   "source": [
    "#word tokenizing of each sentence in train data \n",
    "nest_train=[]\n",
    "for sent in data_train:\n",
    "  temp = re.split(r'[\" \"]+', sent)\n",
    "  for i in temp:\n",
    "    if i==\"\":\n",
    "      temp.remove(i)\n",
    "  nest_train.append(temp)\n",
    "for i in nest_train:\n",
    "  if i==[\"\"]:\n",
    "    nest_train.remove(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Acm8gOAENv9t"
   },
   "outputs": [],
   "source": [
    "#word tokenizing of each sentence in test data \n",
    "nest_test=[]\n",
    "for sent in data_test:\n",
    "  temp = re.split(r'[\" \"]+', sent)\n",
    "  for i in temp:\n",
    "    if i==\"\":\n",
    "      temp.remove(i)\n",
    "  nest_test.append(temp)\n",
    "for i in nest_test:\n",
    "  if i==[\"\"]:\n",
    "    nest_test.remove(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j5FipGKqGAJA"
   },
   "outputs": [],
   "source": [
    "def getwordngrams(processed_tokenized_tweet, k):\n",
    "\tword_n_grams = []\n",
    "  \n",
    "\tfor i in range(k, k + 1):\n",
    "\t\tword_i_grams = [\" \".join(processed_tokenized_tweet[j:j+i]) for j in range(len(processed_tokenized_tweet) - (i-1))]\n",
    "\t\tword_n_grams= word_i_grams + word_n_grams\n",
    "\treturn word_n_grams\n",
    "unigram =[]\n",
    "tempp=[]\n",
    "for d in nest_pro:\n",
    "    tempp.append(getwordngrams(d,1))\n",
    "for l in tempp:\n",
    "    unigram+=l\n",
    "unidict=Counter(unigram)\n",
    "tokens=0\n",
    "for i in unidict.keys():\n",
    "    tokens+=unidict[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ouarquetLR0b"
   },
   "outputs": [],
   "source": [
    "bigram =[]\n",
    "tempp=[]\n",
    "for d in nest_pro:\n",
    "  tempp.append(getwordngrams(d,2))\n",
    "for l in tempp:\n",
    "  bigram+=l\n",
    "bidict=Counter()\n",
    "for i in range(len(unigram)-1):\n",
    "    b=(unigram[i],unigram[i+1])\n",
    "    bidict[b]=bidict.get(b,0)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KDTm9RXyLSDv"
   },
   "outputs": [],
   "source": [
    "trigram =[]\n",
    "tempp=[]\n",
    "for d in nest_pro:\n",
    "  tempp.append(getwordngrams(d,3))\n",
    "for l in tempp:\n",
    "  trigram+=l\n",
    "tridict=Counter()\n",
    "for i in range(len(unigram)-2):\n",
    "    b=(unigram[i],unigram[i+1],unigram[i+2])\n",
    "    tridict[b]=tridict.get(b,0)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3_CejzymLeii"
   },
   "outputs": [],
   "source": [
    "quadgram =[]\n",
    "tempp=[]\n",
    "for d in nest_pro:\n",
    "  tempp.append(getwordngrams(d,4))\n",
    "for l in tempp:\n",
    "  quadgram+=l\n",
    "quaddict=Counter()\n",
    "for i in range(len(unigram)-3):\n",
    "    b=(unigram[i],unigram[i+1],unigram[i+2],unigram[i+3])\n",
    "    quaddict[b]=quaddict.get(b,0)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 249
    },
    "colab_type": "code",
    "id": "bYAEUyMSTF_L",
    "outputId": "68c451f5-668e-4844-91e4-3996bf772828"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: unigram mle for alice: 2.903979904459061e-06\n",
      "Example: unigram mle for 'give away': 0.011965811965811967\n"
     ]
    }
   ],
   "source": [
    "umle={}\n",
    "for i in unidict.keys():\n",
    "    umle[i]=unidict[i]/tokens\n",
    "#example\n",
    "print(\"Example: unigram mle for alice:\",umle[\"alice\"])\n",
    "\n",
    "#bigrams\n",
    "bmle={}\n",
    "for (i,j) in bidict.keys():\n",
    "    bmle[(i,j)]=bidict[(i,j)]/unidict[i]\n",
    "#print(big_mle) \n",
    "#example\n",
    "print(\"Example: unigram mle for 'give away':\",bmle['give', 'away'])\n",
    "\n",
    "#trigrams\n",
    "tmle={}\n",
    "for (i,j,k) in tridict.keys():\n",
    "    tmle[(i,j,k)]=tridict[(i,j,k)]/bidict[(i,j)]\n",
    "#quadgrams\n",
    "qmle={}\n",
    "for (i,j,k,l) in quaddict.keys():\n",
    "    qmle[(i,j,k,l)]=quaddict[(i,j,k,l)]/tridict[(i,j,k)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "nH234nHlLxyW",
    "outputId": "f88494ee-bcc2-4d41-c7e8-67d6a3c12d68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Actual     Possible            \n",
      "          ------     --------            \n",
      "unigram   14877      14877               \n",
      "bigram    201191     221325129           \n",
      "trigram   281915     3292653944133       \n",
      "quadgram  330675     48984812726866641   \n"
     ]
    }
   ],
   "source": [
    "#How many n-grams are possible and how many actually exist?\n",
    "to=len(unidict)\n",
    "\n",
    "print(\"          {0: <10} {1: <20}\".format(\"Actual\", \"Possible\"))\n",
    "print(\"          {0: <10} {1: <20}\".format(\"------\", \"--------\"))\n",
    "print(\"unigram   {0: <10} {1: <20}\".format(to,to))\n",
    "print(\"bigram    {0: <10} {1: <20}\".format(len(bidict), to**2))\n",
    "print(\"trigram   {0: <10} {1: <20}\".format(len(tridict), to**3))\n",
    "print(\"quadgram  {0: <10} {1: <20}\".format(len(quaddict), to**4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "P3R-svik1zaH",
    "outputId": "6c5000d6-e380-496a-bf85-729a55efad07"
   },
   "outputs": [],
   "source": [
    "def findnextword(prob,next_word): \n",
    "    s=0\n",
    "    for i in prob:\n",
    "        s+=i\n",
    "    for i in prob:\n",
    "        i=i/s\n",
    "    a=np.random.multinomial(1,prob,size=1).tolist()\n",
    "    return next_word[a[0].index(1)]\n",
    "def bigprediction(cw):\n",
    "    prob=[]\n",
    "    next_word=[]\n",
    "    for (b1,b2) in bmle.keys():\n",
    "        if (b1==cw):\n",
    "            prob.append(bmle[(b1,b2)])\n",
    "            next_word.append(b2)\n",
    "    return prob,next_word\n",
    "\n",
    "def trigprediction(cw,cw1):\n",
    "    prob=[]\n",
    "    next_word=[]\n",
    "    for (t1,t2,t3) in tmle.keys():\n",
    "        if ((t1,t2)==(cw,cw1)):\n",
    "            prob.append(tmle[(t1,t2,t3)])\n",
    "            next_word.append(t3)\n",
    "    return prob,next_word\n",
    "\n",
    "def quadprediction(cw,cw1,cw2):\n",
    "    prob=[]\n",
    "    next_word=[]\n",
    "    for (q1,q2,q3,q4) in qmle.keys():\n",
    "        if ((q1,q2,q3)==(cw,cw1,cw2)):\n",
    "            prob.append(qmle[(q1,q2,q3,q4)])\n",
    "            next_word.append(q4)\n",
    "    return prob,next_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram sentences:\n",
      "1.appear prominently whenever spoke seemed resolved instead going farther \n",
      "2.oh \n",
      "3.whatever may like emma quite unawares quite unpardonable folly conduct \n",
      "4.allowed promote father lived miles \n",
      "5.catherine s spirits would different ways love suit be\n",
      "\n",
      " Trigram sentences:\n",
      "1.arrival seemed afford real satisfaction good heart settling family females cottage satisfaction sportsman sportsman though esteems sex sportsmen likewise o\n",
      "2.fanny led willingly though impossible fanny s head therefore oppose mother s sake forbear recalling remembrance unhappy situation repaired impris\n",
      "3.sisters intended walk said mary \n",
      "4.let us quarrel past \n",
      "5.henrietta exactly state recentlyimproved views freshformed happiness \n",
      "\n",
      " Quadgram sentences:\n",
      "1.elizabeth excessively disappointed set heart seeing lakes still thought might time enough \n",
      "2.also handsome replied elizabeth young man ought \n",
      "3.heartiness warmth sincerity anne delighted sad want blessings home \n",
      "4.every body dearest always hand probably never shall therefore till outlived affect\n",
      "5.knew s opinion either constancy change one leading point anne s conduct sub\n"
     ]
    }
   ],
   "source": [
    "def sent_gen(n): #n=2 for bigram, 3 for trigram..\n",
    "    sent_length=random.randint(8,20) #length of sentence\n",
    "    sentence=[\"<s>\"]\n",
    "    length=0\n",
    "    cw,cw1,cw2=\"<s>\",\"\",\"\"  #current words 0,1,2 for predicting next word\n",
    "    \n",
    "    if(n==2):\n",
    "        while(length<sent_length):\n",
    "            prob,next_word=bigprediction(cw)\n",
    "            f=findnextword(prob,next_word)\n",
    "            \n",
    "            sentence.append(f)\n",
    "            cw=f\n",
    "            length+=1\n",
    "            if(cw==\"</s>\"):\n",
    "                break\n",
    "    \n",
    "    if(n==3):\n",
    "        prob,next_word=bigprediction(cw)  #predict first word using bigram\n",
    "        f=findnextword(prob,next_word)\n",
    "        sentence.append(f)\n",
    "        cw1=f\n",
    "        length+=1\n",
    "        \n",
    "        while(length<sent_length): #predict all other words using trigram\n",
    "            prob,next_word=trigprediction(cw,cw1)\n",
    "            f=findnextword(prob,next_word)\n",
    "            sentence.append(f)\n",
    "            cw=cw1\n",
    "            cw1=f\n",
    "            length+=1\n",
    "            if(cw1==\"</s>\"):\n",
    "                break\n",
    "    if(n==4):\n",
    "        prob,next_word=bigprediction(cw) #predict first word using bigram\n",
    "        f=findnextword(prob,next_word)\n",
    "        sentence.append(f)\n",
    "        cw1=f\n",
    "        \n",
    "        prob,next_word=trigprediction(cw,cw1) #predict second word using trigram\n",
    "        f=findnextword(prob,next_word)\n",
    "        sentence.append(f)\n",
    "        cw2=f\n",
    "        length+=2\n",
    "        \n",
    "        while(length<sent_length): #predict all other words using quadgram\n",
    "            prob,next_word=quadprediction(cw,cw1,cw2)\n",
    "            f=findnextword(prob,next_word)\n",
    "            sentence.append(f)\n",
    "            cw=cw1\n",
    "            cw1=cw2\n",
    "            cw2=f\n",
    "            length+=1\n",
    "            if(cw2==\"</s>\"):\n",
    "                break\n",
    "    s=\" \".join(sentence)\n",
    "    return s\n",
    "print(\"Bigram sentences:\")        \n",
    "for i in range(5):\n",
    "    a=sent_gen(2)\n",
    "    print(str(i+1)+'.'+a[4:len(a)-4])\n",
    "print(\"\\n Trigram sentences:\")\n",
    "for i in range(5):\n",
    "    a=sent_gen(3) \n",
    "    print(str(i+1)+'.'+a[4:len(a)-4])\n",
    "\n",
    "print(\"\\n Quadgram sentences:\")  \n",
    "for i in range(5):\n",
    "    a=sent_gen(4)\n",
    "    print(str(i+1)+'.'+a[4:len(a)-4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The statements are grammatically senseless. the sentences get smaller with increase in 'n' inn ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_test=[]\n",
    "for sent in data_test:\n",
    "    temp = re.split(r'[\" \"]+', sent)\n",
    "    for i in temp:\n",
    "        if i==\"\":\n",
    "            temp.remove(i)\n",
    "    nest_test.append(temp)\n",
    "test_words =[]\n",
    "tempp=[]\n",
    "for d in nest_test:\n",
    "    tempp.append(getwordngrams(d,1))\n",
    "for l in tempp:\n",
    "    test_words+=l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(n,sentence):\n",
    "    prob=0\n",
    "    for i in range(len(sentence)-n):\n",
    "        try:\n",
    "            if(n==1):\n",
    "                prob+=np.log2(umle[sentence[i]])\n",
    "            if(n==2):\n",
    "                prob+=np.log2(bmle[(sentence[i],sentence[i+1])])\n",
    "            if(n==3):\n",
    "                prob+=np.log2(tmle[(sentence[i],sentence[i+1],sentence[i+2])])\n",
    "            if(n==4):\n",
    "                prob+=np.log2(qmle[(sentence[i],sentence[i+1],sentence[i+2],sentence[i+3])])\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    l =  prob/len(sentence)\n",
    "    return np.power(2, -l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_mean_perplexity(n, test_dataset):\n",
    "    test_per=0\n",
    "    for sentence in test_dataset:\n",
    "        test_per+=perplexity(n,sentence)\n",
    "    return test_per/len(test_dataset)\n",
    "\n",
    "def test_perplexity(n, test_dataset):\n",
    "    test_per=0\n",
    "    for sentence in test_dataset:\n",
    "        test_per+=perplexity(n,sentence)\n",
    "    return test_per"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Perplexity           Mean Perplexity     \n",
      "           ----------           ---------------\n",
      " unigram   5407911.921802663    771.2367258703171\n",
      " bigrams   59922.37882859499    8.545690078236593\n",
      " trigrams  8541.955119832657    1.2181909754467566\n",
      " quadgrams 7066.518793681491    1.0077750704052326\n"
     ]
    }
   ],
   "source": [
    "token_sent_test = nest_test\n",
    "print(\"           {0: <20} {1: <20}\".format(\"Perplexity\", \"Mean Perplexity\"))\n",
    "print(\"           {0: <20} {1: <10}\".format(\"----------\", \"---------------\"))\n",
    "print(\" unigram   {0: <20} {1: <10}\".format(test_perplexity(1, token_sent_test),test_mean_perplexity(1, token_sent_test)))\n",
    "print(\" bigrams   {0: <20} {1: <10}\".format(test_perplexity(2, token_sent_test),test_mean_perplexity(2, token_sent_test)))\n",
    "print(\" trigrams  {0: <20} {1: <10}\".format(test_perplexity(3, token_sent_test),test_mean_perplexity(3, token_sent_test)))\n",
    "print(\" quadgrams {0: <20} {1: <10}\".format(test_perplexity(4, token_sent_test),test_mean_perplexity(4, token_sent_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Neural Network will mostly have a better output than the classical appoarch. This is because it can tackle sentences that it has never seen. It also uses context of all of the previous words in the sentences, hence it would be better then classical approach "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Assignment_2_17110049_Dhaiwat_Kabaria_draft.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
